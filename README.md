# Tri-Attention <br> (Official PyTorch Implementation)


-------

This repository is the official PyTorch implementation of the paper "[Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural Language Processing](https://doi.org/*****)" is submitting to TNNLS by _Rui Yu, Yifeng Li, Wenpeng Lu and Longbing Cao_. 


-------
&nbsp;
## Brief Introduction

In natural language processing (NLP), the context of a word or sentence plays an essential role. Contextual information such as the semantic representation of a passage or historical dialogue forms an essential part of a conversation and a precise understanding of the present phrase or sentence. However, the standard attention mechanisms typically generate weights using query and key but ignore context, forming a Bi-Attention framework, despite their great success in modeling sequence alignment. This Bi-Attention mechanism does not explicitly model the interactions between the contexts, queries and keys of target sequences, missing important contextual information and resulting in poor attention performance. Accordingly, a novel and general triple-attention (Tri-Attention) framework expands the standard Bi-Attention mechanism and explicitly interacts query, key, and context by incorporating context as the third dimension in calculating relevance scores. Four variants of Tri-Attention are generated by expanding the two-dimensional vector-based additive, dot-product, scaled dot-product, and bilinear operations in Bi-Attention to the tensor operations for Tri-Attention. Extensive experiments on three NLP tasks demonstrate that Tri-Attention outperforms about 30 state-of-the-art non-attention, standard Bi-Attention, contextual Bi-Attention approaches and pretrained neural language models.


-------
&nbsp;

## Retrieval-based dialogues

### Installation

#### Pre-Requisites
You must have NVIDIA GPUs to run the codes.

The implementation codes are developed and tested with the following environment setups:
- numpy==1.19.5
- setproctitle==1.2.2
- torch==1.8.0.dev20210113+cu110
- torchvision==0.9.0.dev20210113+cu110
- tqdm==4.56.2
- transformers==2.8.0

We recommend using the exact setups above.

#### Code Installation

This code is implemented using PyTorch v1.8.0, and provides out of the box support with CUDA 11.2
Anaconda is the recommended to set up this codebase.
```
# https://pytorch.org
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge
pip install -r requirements.txt
```

#### Preparing Data and Checkpoints
-------------

#### Post-trained and fine-tuned Checkpoints

We provide following post-trained and fine-tuned checkpoints. 

- [fine-grained post-trained checkpoint for ubuntu benchmark dataset][3]
- [fine-tuned checkpoint for ubuntu benchmark dataset ][4]


#### Data pkl for Fine-tuning (Response Selection)
We used the following data for post-training and fine-tuning
- [fine-grained post-training dataset and fine-tuning dataset for ubuntu benchmark ][5]


Original version for ubuntu dataset is availble in [Ubuntu Corpus V1][6].

----------

### Usage

##### Making Data for post-training and fine-tuning  

```
Data_processing.py
```


### Post-training Example
```shell
python -u FPT/ubuntu_final.py --num_train_epochs 25
```

### Fine-tuning Example

###### Taining 
```shell
To train the model, set `--is_training`
python -u Fine-Tuning/Response_selection.py --task ubuntu --is_training
```
###### Testing
```shell
python -u Fine-Tuning/Response_selection.py --task ubuntu
```


## Sentence semantic matching

### Installation

#### Pre-Requisites
You must have NVIDIA GPUs to run the codes.

The implementation codes are developed and tested with the following environment setups:
- python3.6
- torch == 1.5.1
- tqdm == 4.46.0
- scikit-learn == 0.23.1
- numpy == 1.19.4
- transformers == 3.4.0
- nltk==3.5

We recommend using the exact setups above. However, other environments (Linux, Python>=3.6, CUDA>=9.2, GCC>=5.4, PyTorch>=1.5.1, TorchVision>=0.6.1) should also work properly.

#### Code Installation

This code is implemented using PyTorch v1.8.0, and provides out of the box support with CUDA 11.2
Anaconda is the recommended to set up this codebase.
```
# https://pytorch.org
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge
pip install -r requirements.txt
```

#### Preparing Data and Checkpoints
-------------

#### Dataset and pre-trained language model

The LCQMC dataset is provided in [LCQMC][9]. 

The pre-trained language model BERT is provided in [BERT-base-chinese](https://huggingface.co/bert-base-chinese#).

----------

### Usage

#### Taining & Testing
```shell
python -u SPM/run.py
```


## Machine reading comprehension

### Installation

#### Pre-Requisites
You must have NVIDIA GPUs to run the codes.

The implementation codes are developed and tested with the following environment setups:
- pytorch-lightning==1.2.3
- transformers==4.3.3
- datasets==1.4.1
- optuna==2.6.0
- torch==1.8.0


#### Code Installation

This code is implemented using PyTorch v1.8.0, and provides out of the box support with CUDA 11.2
Anaconda is the recommended to set up this codebase.
```
# https://pytorch.org
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge
pip install -r requirements.txt
```

#### Preparing Data and Checkpoints
-------------

#### Dataset and pre-trained language model

The RACE dataset is provided in [RACE](http://www.cs.cmu.edu/~glai1/data/race/). 

The pre-trained language model BERT is provided in [BERT-base-uncased](https://huggingface.co/models).


----------

### Usage

#### Taining & Testing
```shell
python3 -u RC/race.py --data_dir=RC/RACE 
                      --bert_model=RC/bert-base-uncased 
                      --output_dir=RC/base_models 
                      --max_seq_length=512 
                      --do_train 
                      --do_eval 
                      --test  
                      --do_lower_case 
                      --train_batch_size=16 
                      --eval_batch_size=4 
                      --test_batch_size=4 
                      --learning_rate=2e-5 
                      --num_train_epochs=10 
                      --gradient_accumulation_steps=16 
                      --loss_scale=0
```



----------

&nbsp;
## Citation

If you find Tri-Attention useful or inspiring, please consider citing:

```bibtex
@article{Tri-Attention-2022,
  author={Rui Yu, Yifeng Li, Wenpeng Lu and Longbing Cao},
  journal={****}, 
  title={Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural Language Processing}, 
  year={2022},
}
```

----------
&nbsp;
## Acknowledgement

Our proposed Tri-Attention is heavily inspired by many outstanding prior works, including [COIN](https://aclanthology.org/2021.emnlp-main.312.pdf).



[1]: https://github.com/huggingface/transformers
[2]: https://github.com/taesunwhang/BERT-ResSel
[3]: https://drive.google.com/file/d/1-4E0eEjyp7n_F75TEh7OKrpYPK4GLNoE/view?usp=sharing
[4]: https://drive.google.com/file/d/1n2zigNDiIArWtsiV9iUQLwfSBgtNn7ws/view?usp=sharing
[5]: https://drive.google.com/file/d/16Rv8rSRneq7gfPRkpFZseNYfswuoqI4-/view?usp=sharing
[6]: https://www.dropbox.com/s/2fdn26rj6h9bpvl/ubuntu_data.zip
[7]: https://github.com/MarkWuNLP/MultiTurnResponseSelection
[8]: https://github.com/cooelf/DeepUtteranceAggregation
[9]: https://www.aclweb.org/anthology/C18-1166/
